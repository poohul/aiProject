import numpy as np
from transformers import GPT2Tokenizer, GPT2Model
import faiss
import torch

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2Model.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# í…ìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì œ
texts = [
    "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€?",
    "ê¸°ê³„í•™ìŠµì˜ ê¸°ì´ˆ.",
    "ìì—°ì–´ì²˜ë¦¬ì˜ ìµœì‹  ë™í–¥.",
    "ë”¥ëŸ¬ë‹ìœ¼ë¡œ í•˜ëŠ” ì´ë¯¸ì§€ ë¶„ë¥˜.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” 21ë„ì…ë‹ˆë‹¤",
    "í™”ì°½í•œ ë‚ ì”¨ê°€ ê³„ì† ë©ë‹ˆë‹¤",
]


def embed_text(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )

    with torch.no_grad():
        outputs = model(**inputs)

    attention_mask = inputs['attention_mask']
    embeddings = outputs.last_hidden_state

    masked_embeddings = embeddings * attention_mask.unsqueeze(-1)
    summed = masked_embeddings.sum(dim=1)
    lengths = attention_mask.sum(dim=1).unsqueeze(-1)
    mean_embeddings = summed / lengths

    return mean_embeddings.detach().numpy()


def cosine_similarity(v1, v2):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    v1_flat = v1.flatten()
    v2_flat = v2.flatten()
    return np.dot(v1_flat, v2_flat) / (
            np.linalg.norm(v1_flat) * np.linalg.norm(v2_flat)
    )


def smart_search(query, similarity_threshold=0.3):
    """ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰"""
    print(f"ì§ˆë¬¸: '{query}'")

    # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
    query_vec = embed_text(query)

    # ëª¨ë“  ë¬¸ì„œì™€ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for i, text in enumerate(texts):
    #     for i, text in enumerate(1):
        doc_vec = embed_text(text)
        sim = cosine_similarity(query_vec, doc_vec)
        similarities.append((i, sim, text))

    # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)

    print(f"\n=== ëª¨ë“  ë¬¸ì„œì˜ ìœ ì‚¬ë„ ===")
    for i, (idx, sim, text) in enumerate(similarities):
        print(f"{i + 1}. ìœ ì‚¬ë„: {sim:.4f} - '{text}'")

    # ì„ê³„ê°’ ì²´í¬
    best_similarity = similarities[0][1]

    if best_similarity < similarity_threshold:
        print(f"\nâš ï¸  ìµœê³  ìœ ì‚¬ë„({best_similarity:.4f})ê°€ ì„ê³„ê°’({similarity_threshold})ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤.")
        print("âŒ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì‹œê±°ë‚˜, ë‹¤ìŒ ì£¼ì œë“¤ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”:")
        for text in texts:
            print(f"   - {text}")
        return None
    else:
        print(f"\nâœ… ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤! (ìœ ì‚¬ë„: {best_similarity:.4f})")
        print(f"ğŸ“„ ê°€ì¥ ê´€ë ¨ëœ ë¬¸ì„œ: '{similarities[0][2]}'")
        return similarities[0]


def test_various_queries():
    """ë‹¤ì–‘í•œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"""
    test_queries = [
        # "ê¸°ê³„í•™ìŠµì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",  # ê´€ë ¨ ìˆëŠ” ì§ˆë¬¸
        # "ë”¥ëŸ¬ë‹ ì´ë¯¸ì§€ ë¶„ë¥˜",  # ê´€ë ¨ ìˆëŠ” ì§ˆë¬¸
        # "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",  # ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸
        # "ì ì‹¬ ë©”ë‰´ ì¶”ì²œí•´ì£¼ì„¸ìš”",  # ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸
        # "ìì—°ì–´ì²˜ë¦¬ ë™í–¥",  # ê´€ë ¨ ìˆëŠ” ì§ˆë¬¸
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
    ]

    print("=== ë‹¤ì–‘í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ===\n")

    for query in test_queries:
        result = smart_search(query, similarity_threshold=0.3)
        print("-" * 50)


def adjust_threshold_test():
    """ì„ê³„ê°’ ì¡°ì • í…ŒìŠ¤íŠ¸"""
    query = "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?"
    thresholds = [0.1, 0.3, 0.5, 0.7]

    print(f"=== ì„ê³„ê°’ ì¡°ì • í…ŒìŠ¤íŠ¸ (ì§ˆë¬¸: '{query}') ===\n")

    for threshold in thresholds:
        print(f"ì„ê³„ê°’: {threshold}")
        result = smart_search(query, similarity_threshold=threshold)
        print("-" * 30)


if __name__ == "__main__":
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    test_various_queries()

    print("\n" + "=" * 60 + "\n")

    # ì„ê³„ê°’ ì¡°ì • í…ŒìŠ¤íŠ¸
    adjust_threshold_test()