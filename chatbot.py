# chatbot.py
import os
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from commonUtil.timeCheck import logging_time
import re

# --------------------------------------------------------
# ğŸ’¬ í”„ë¡¬í”„íŠ¸: ìµœì‹  ë‚ ì§œ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ ì§€ì‹œ
# --------------------------------------------------------
PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ íšŒì‚¬ ê²Œì‹œíŒì˜ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš© ìš”ì•½ì…ë‹ˆë‹¤:
{context}

[ì§€ì‹œì‚¬í•­]
1. ì—¬ëŸ¬ ë¬¸ì„œê°€ ê´€ë ¨ëœ ê²½ìš°, ë°˜ë“œì‹œ metadataì˜ 'date' ê°’ì´ ê°€ì¥ ìµœì‹ ì¸ ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‚ ì§œê°€ ë™ì¼í•œ ê²½ìš° metadataì˜ 'ê²Œì‹œì¼ì‹œ'ë¥¼ ë¹„êµí•˜ì—¬ ìµœì‹  ê²Œì‹œê¸€ì„ ì„ íƒí•˜ì„¸ìš”.
3. ë°˜ë“œì‹œ ì‹¤ì œ ë¬¸ì„œì— ì¡´ì¬í•˜ëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. ê°€ëŠ¥í•˜ë‹¤ë©´ ë‹¤ìŒ ì •ë³´ë¥¼ í•¨ê»˜ í¬í•¨í•˜ì„¸ìš”:
   - ê²Œì‹œì¼ì
   - ê²Œì‹œì
   - ì œëª©
5. ì‚¬ìš©ì ìš”ì²­ì´ 'ì œëª©ì— í¬í•¨ëœ ë‹¨ì–´' ë¥¼ ì°¾ëŠ” ê²½ìš° ë°˜ë“œì‹œ ë¬¸ì„œì˜ 'ì œëª©' í•„ë“œì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼í•¨.  
6. ë‹¤ìŒ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ë‚ ì§œ(date)ê°€ ë©”íƒ€ë°ì´í„°ë¡œ ì œê³µë˜ì–´ ìˆë‹¤ë©´, ë°˜ë“œì‹œ í•´ë‹¹ ë‚ ì§œ(date)ê°€ ì •í™•íˆ
   ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë§Œ ê³¨ë¼ì„œ ë‹µë³€í•˜ì„¸ìš”.ê·¸ ì™¸ì˜ ë‚ ì§œëŠ” ë¬´ì‹œí•˜ì„¸ìš”.
   
[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

ìµœì¢… ìš”ì•½ëœ ë‹µë³€:
"""

@logging_time
def get_answer(qa, query,context):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
    # return qa.invoke({"query": query})["result"]
    # return qa.invoke({"query": query})["result"]
    return qa.invoke({"query": query, "context": context})["result"]

def load_vector_db(persist_dir="./chroma_db2"):
    """ë²¡í„° DB ë¡œë“œ"""
    # embeddings = HuggingFaceEmbeddings(
    #     model_name="upskyy/gte-base-korean",
    #     model_kwargs={'trust_remote_code': True}
    # )
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
    )
    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    return db


# ì œëª© ìœ¼ë¡œ ê²€ìƒ‰ì‹œ í•„í„° ì¶”ê°€
def filter_by_title(query, docs):
    """ì‚¬ìš©ì ì§ˆë¬¸ì— 'ì œëª©' í‚¤ì›Œë“œê°€ ìˆì„ ê²½ìš°, metadata['title'] ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§"""
    title_pattern = re.search(r"ì œëª©.*?(?:ì´|ì—)?\s*([^\s]+)", query)
    if not title_pattern:
        return docs

    keyword = title_pattern.group(1).strip()
    filtered = [d for d in docs if keyword in (d.metadata.get("title") or "")]
    if not filtered:
        return docs  # fallback: ì œëª© ë§¤ì¹­ ì—†ìœ¼ë©´ ì „ì²´ ìœ ì§€
    return filtered

def main():
    print("ğŸ¤– ê²Œì‹œíŒ ê¸°ë°˜ ì±—ë´‡ (Ctrl+C ë¡œ ì¢…ë£Œ)\n")

    # -------------------------
    # â‘  ë²¡í„° DB ë¡œë“œ
    # -------------------------
    db = load_vector_db()
    # retriever = db.as_retriever(search_kwargs={"k": 30})  # k ì¡°ì ˆ ê°€ëŠ¥
    retriever = db.as_retriever(search_kwargs={"k": 20})  # k ì¡°ì ˆ ê°€ëŠ¥
    # -------------------------
    # â‘¡ LLM + í”„ë¡¬í”„íŠ¸ ì •ì˜
    # -------------------------
    # llm = Ollama(model="llama3.1:8b")  # PC ë²„ì „ (ë¹ ë¦„, ì •í™•í•¨)
    # llm = Ollama(model="llama3.2:3b") #ë…¸íŠ¸ë¶ ì‚¬ì–‘ ë¬¸ì œë¡œ ë‚®ì€ ëª¨ë¸ ì‚¬ìš© ì•„.. ë„ˆë¬´ ë©ì²­í•œë°..
    llm = Ollama(
        model="llama3.2:3b",
        # ë˜ëŠ” í•œê¸€ íŠ¹í™” ëª¨ë¸ ê³ ë ¤
        # model="EEVE-Korean-10.8B"
    )

    prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt}
    )

    # -------------------------
    # â‘¢ ëŒ€í™” ë£¨í”„
    # -------------------------
    while True:
        try:
            query = input("\nğŸ—¨ï¸ ì§ˆë¬¸: ").strip()
            if not query:
                continue

            # 1ï¸âƒ£ RAG ê²€ìƒ‰
            docs = retriever.get_relevant_documents(query)

            # 2ï¸âƒ£ ì œëª© ê¸°ë°˜ í•„í„°ë§
            docs = filter_by_title(query, docs)

            # 3ï¸âƒ£ ë‹µë³€ ìƒì„±
            context = ""
            # ë¬¸ë§¥ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
            # context = "\n\n".join([d.page_content for d in docs])
            # context = "\n\n".join([
            #     # f"ì œëª©: {d.metadata.get('title')}\në‚´ìš©: {d.page_content}"
            #     f"ì œëª©: {d.metadata.get('title')}"
            #     for d in docs
            # ])
            # response = qa.run({"question": query, "context": context})
            response = get_answer(qa, query, context)
            # 4ï¸âƒ£ ê²°ê³¼ í‘œì‹œ
            print("\nğŸ’¬ ë‹µë³€:", response)
            print("\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")

            # print("\nâ³ ê²€ìƒ‰ ì¤‘...")
            # answer = get_answer(qa, query)
            # print(f"\nğŸ’¡ ë‹µë³€:\n{answer}")

            # docs = retriever.get_relevant_documents(query)
            for i, doc in enumerate(docs, 1):
                # print(f"  [{i}] {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ'),}")
                print(f"  [{i}] ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} / ë‚ ì§œ: {doc.metadata.get('date', 'ë‚ ì§œ ì—†ìŒ')}")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


if __name__ == "__main__":
    main()
