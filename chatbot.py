# chatbot.py
import os
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from commonUtil.timeCheck import logging_time

# --------------------------------------------------------
# ğŸ’¬ í”„ë¡¬í”„íŠ¸: ìµœì‹  ë‚ ì§œ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ ì§€ì‹œ
# --------------------------------------------------------
PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ íšŒì‚¬ ê²Œì‹œíŒì˜ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš© ìš”ì•½ì…ë‹ˆë‹¤:
{context}

[ì§€ì‹œì‚¬í•­]
1. ì—¬ëŸ¬ ë¬¸ì„œê°€ ê´€ë ¨ëœ ê²½ìš°, ë°˜ë“œì‹œ metadataì˜ 'date' ê°’ì´ ê°€ì¥ ìµœì‹ ì¸ ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‚ ì§œê°€ ë™ì¼í•œ ê²½ìš° metadataì˜ 'ê²Œì‹œì¼ì‹œ'ë¥¼ ë¹„êµí•˜ì—¬ ìµœì‹  ê²Œì‹œê¸€ì„ ì„ íƒí•˜ì„¸ìš”.
3. ë°˜ë“œì‹œ ì‹¤ì œ ë¬¸ì„œì— ì¡´ì¬í•˜ëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. ê°€ëŠ¥í•˜ë‹¤ë©´ ë‹¤ìŒ ì •ë³´ë¥¼ í•¨ê»˜ í¬í•¨í•˜ì„¸ìš”:
   - ê²Œì‹œì¼ì
   - ê²Œì‹œì
   - ì œëª©
5.ì‚¬ìš©ì ìš”ì²­ì´ 'ì œëª©ì— í¬í•¨ëœ ë‹¨ì–´' ë¥¼ ì°¾ëŠ” ê²½ìš° ë°˜ë“œì‹œ ë¬¸ì„œì˜ 'ì œëª©' í•„ë“œì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼í•¨.  
6.ë‹¤ìŒ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ë‚ ì§œ(date)ê°€ ë©”íƒ€ë°ì´í„°ë¡œ ì œê³µë˜ì–´ ìˆë‹¤ë©´,
ë°˜ë“œì‹œ í•´ë‹¹ ë‚ ì§œ(date)ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë§Œ ê³¨ë¼ì„œ ë‹µë³€í•˜ì„¸ìš”.
ê·¸ ì™¸ì˜ ë‚ ì§œëŠ” ë¬´ì‹œí•˜ì„¸ìš”.
[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

ìµœì¢… ìš”ì•½ëœ ë‹µë³€:
"""

@logging_time
def get_answer(qa, query):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
    return qa.invoke({"query": query})["result"]

def load_vector_db(persist_dir="./chroma_db2"):
    """ë²¡í„° DB ë¡œë“œ"""
    embeddings = HuggingFaceEmbeddings(
        model_name="upskyy/gte-base-korean",
        model_kwargs={'trust_remote_code': True}
    )
    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    return db

def main():
    print("ğŸ¤– ê²Œì‹œíŒ ê¸°ë°˜ ì±—ë´‡ (Ctrl+C ë¡œ ì¢…ë£Œ)\n")

    # -------------------------
    # â‘  ë²¡í„° DB ë¡œë“œ
    # -------------------------
    db = load_vector_db()
    retriever = db.as_retriever(search_kwargs={"k": 30})  # k ì¡°ì ˆ ê°€ëŠ¥

    # -------------------------
    # â‘¡ LLM + í”„ë¡¬í”„íŠ¸ ì •ì˜
    # -------------------------
    llm = Ollama(model="llama3.1:8b")  # PC ë²„ì „ (ë¹ ë¦„, ì •í™•í•¨)

    prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt}
    )

    # -------------------------
    # â‘¢ ëŒ€í™” ë£¨í”„
    # -------------------------
    while True:
        try:
            query = input("\nğŸ—¨ï¸ ì§ˆë¬¸: ").strip()
            if not query:
                continue

            print("\nâ³ ê²€ìƒ‰ ì¤‘...")
            answer = get_answer(qa, query)
            print(f"\nğŸ’¡ ë‹µë³€:\n{answer}")

            docs = retriever.get_relevant_documents(query)
            for i, doc in enumerate(docs, 1):
                # print(f"  [{i}] {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ'),}")
                print(f"  [{i}] ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} / ë‚ ì§œ: {doc.metadata.get('date', 'ë‚ ì§œ ì—†ìŒ')}")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


if __name__ == "__main__":
    main()
