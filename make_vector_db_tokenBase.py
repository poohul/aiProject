import os
import json
from pathlib import Path
from tqdm import tqdm
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from commonUtil.timeCheck import logging_time
from transformers import AutoTokenizer
# ë‚ ì§œ ì²˜ë¦¬ë¥¼ ìœ„í•´ datetime ëª¨ë“ˆ ì¶”ê°€
from datetime import datetime

# ---------- ì „ì—­ ì„¤ì • (í† í° ê¸°ë°˜ ë¶„í•  ê¸°ì¤€) ----------
TOKEN_CHUNK_SIZE = 500
TOKEN_CHUNK_OVERLAP = 100
EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
DB_FOLDER = "./chroma_db3"
# ê²Œì‹œì¼ì‹œ í¬ë§· ì •ì˜ (í˜„ì¬ ì œê³µëœ í¬ë§·: 2025-09-22 09:24:27)
DATE_TIME_FORMAT = "%Y-%m-%d %H:%M:%S"
# ----------------------------------------------------

# í† í¬ë‚˜ì´ì €ë¥¼ ì „ì—­ì ìœ¼ë¡œ ë¡œë“œ
try:
    TOKENIZER = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
except Exception as e:
    print(f"Error loading tokenizer: {e}. Ensure 'transformers' is installed.")
    TOKENIZER = None


def extract_text_from_file(file_path: str) -> dict:
    """íŒŒì¼ì„ ì½ì–´ JSONì´ë©´ ë³€í™˜, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ ë¦¬í„´"""
    # (ê¸°ì¡´ ì½”ë“œëŠ” ë³€ê²½ ì—†ìŒ)
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read().strip()

    if not content:
        return {"text": "", "date": "", "title": "", "body": ""}

    if content.startswith("{") and content.endswith("}"):
        try:
            data = json.loads(content)
            title = data.get("ì œëª©", "")
            body = data.get("ë³¸ë¬¸", "")
            author = data.get("ê²Œì‹œì", "")
            date = data.get("ê²Œì‹œì¼ì‹œ", "") # <-- ì—¬ê¸°ì„œ ë‚ ì§œ ë¬¸ìì—´ íšë“

            text = f"ì œëª©: {title}\në‚´ìš©: {body}\nê²Œì‹œì: {author}\nê²Œì‹œì¼ì‹œ: {date}"

            return {
                "text": text.strip(),
                "date": date,
                "title": title,
                "body": body
            }

        except Exception:
            return {"text": content, "date": "", "title": "", "body": ""}
    else:
        return {"text": content, "date": "", "title": "", "body": ""}


def chunk_text_by_token(text, chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=TOKEN_CHUNK_OVERLAP):
    """ë³¸ë¬¸ì„ í† í° ê¸°ë°˜ìœ¼ë¡œ ë‚˜ëˆ ì£¼ëŠ” chunking í•¨ìˆ˜ (HuggingFace Tokenizer ì‚¬ìš©)"""
    if not TOKENIZER:
        return [text]  # í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¶„í• í•˜ì§€ ì•Šê³  ì›ë³¸ ë°˜í™˜

    # í…ìŠ¤íŠ¸ë¥¼ í† í° ID ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    tokens = TOKENIZER.encode(text, add_special_tokens=False)

    chunks = []

    # í† í° IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŠ¬ë¼ì´ì‹±
    for i in range(0, len(tokens), chunk_size - chunk_overlap):
        # ì²­í¬ì˜ ì‹œì‘ê³¼ ë ì¸ë±ìŠ¤
        start_idx = i
        end_idx = i + chunk_size

        # í† í° ID ì²­í¬ ì¶”ì¶œ
        token_chunk = tokens[start_idx:end_idx]

        # í† í° ID ì²­í¬ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        chunk_text = TOKENIZER.decode(token_chunk)

        chunks.append(chunk_text)

        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬ í›„ ë£¨í”„ ì¢…ë£Œ
        if end_idx >= len(tokens):
            break

    return chunks


def load_documents_from_folder(folder_path: str):
    """í´ë” ë‚´ ëª¨ë“  txt íŒŒì¼ì„ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œ (chunking ì ìš©)"""
    documents = []

    print("ğŸ” Scanning for .txt files...")
    all_txt_files = [
        os.path.join(root, f)
        for root, _, files in os.walk(folder_path)
        for f in files if f.lower().endswith(".txt")
    ]

    if not all_txt_files:
        print("âš ï¸ No .txt files found.")
        return documents

    print(f"ğŸ“Š Found {len(all_txt_files)} .txt files\n")

    for file_path in tqdm(all_txt_files, desc="ğŸ“– Loading & parsing files", unit="file"):
        try:
            result = extract_text_from_file(file_path)

            text = result["text"]
            date_str = result["date"] # ì›ë³¸ ë‚ ì§œ ë¬¸ìì—´
            title = result["title"]
            body = result["body"]

            if not text:
                tqdm.write(f"  âš ï¸ Empty file skipped: {file_path}")
                continue

            # â­â­â­ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: ë‚ ì§œ ë¬¸ìì—´ì„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜ â­â­â­
            date_timestamp = 0.0
            if date_str:
                try:
                    # '2025-09-22 09:24:27' í¬ë§· íŒŒì‹±
                    dt_obj = datetime.strptime(date_str, DATE_TIME_FORMAT)
                    date_timestamp = dt_obj.timestamp() # floatí˜• íƒ€ì„ìŠ¤íƒ¬í”„
                except ValueError:
                    tqdm.write(f"  âš ï¸ Invalid date format in {file_path}. Storing 0.0.")
                    date_timestamp = 0.0 # íŒŒì‹± ì‹¤íŒ¨ ì‹œ 0.0ìœ¼ë¡œ ì €ì¥

            # âœ… ì œëª©ë§Œ ë”°ë¡œ ë²¡í„°í™”
            if title:
                doc_title = Document(
                    page_content=title,
                    metadata={
                        "source": file_path,
                        # â­ date í•„ë“œì— íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                        "date": date_timestamp,
                        "title": title,
                        "type": "title"
                    },
                )
                documents.append(doc_title)

            # âœ… ë³¸ë¬¸ chunking ì ìš© (í† í° ê¸°ë°˜ í•¨ìˆ˜ ì‚¬ìš©)
            chunks = []
            if body:
                chunks = chunk_text_by_token(body, chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=TOKEN_CHUNK_OVERLAP)

                for i, chunk in enumerate(chunks):
                    doc_body = Document(
                        page_content=chunk,
                        metadata={
                            "source": file_path,
                            # â­ date í•„ë“œì— íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                            "date": date_timestamp,
                            "title": title,
                            "chunk_index": i,
                            "type": "body"
                        },
                    )
                    documents.append(doc_body)

            rel_path = os.path.relpath(file_path, folder_path)
            tqdm.write(f"  âœ… {rel_path} (chunks: {len(chunks) if body else 0}, title added: {'Y' if title else 'N'}, date stored as timestamp: {'Y' if date_timestamp else 'N'})")

        except Exception as e:
            tqdm.write(f"  âŒ Error reading {file_path}: {e}")

    print(f"\nâœ¨ Successfully loaded: {len(documents)} entries (chunked body + title í¬í•¨)")
    return documents


@logging_time
def create_vector_db(folder_path, persist_dir=DB_FOLDER):
    print(f"\n{'=' * 60}")
    print(f"ğŸ“‚ Target folder: {folder_path}")
    print(f"ğŸ’¾ Vector DB will be saved to: {persist_dir}")
    print(f"{'=' * 60}\n")

    documents = load_documents_from_folder(folder_path)

    if not documents:
        print("âš ï¸ No valid .txt or JSON files found. Aborting.")
        return None

    # í† í° ê¸°ë°˜ìœ¼ë¡œ ë¶„í• í–ˆê¸° ë•Œë¬¸ì—, í‰ê·  ê¸¸ì´ëŠ” í† í° ìˆ˜(500)ì— ê·¼ì ‘í•  ê²ƒì„.
    avg_length = sum(len(TOKENIZER.encode(doc.page_content, add_special_tokens=False)) for doc in documents) / len(
        documents)
    print(f"ğŸ“Š Average chunk length: {avg_length:.1f} tokens")

    print("\nğŸ” Creating embeddings (this may take a while)...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

    print("ğŸ’¾ Saving to Chroma vector DB...")
    # ChromaDBëŠ” ì´ì œ ë©”íƒ€ë°ì´í„°ì˜ date í•„ë“œë¥¼ floatìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.
    db = Chroma.from_documents(documents, embeddings, persist_directory=persist_dir)

    print(f"\n{'=' * 60}")
    print(f"âœ… Vector DB successfully created!")
    print(f"ğŸ“ Location: {persist_dir}")
    print(f"ğŸ“Š Total embedded entries: {len(documents)} (chunked body + title í¬í•¨)")
    print(f"ğŸ“ˆ Average chunk length: {avg_length:.1f} tokens")
    print(f"{'=' * 60}")
    return db


if __name__ == "__main__":
    folder_path = input("ğŸ“ Enter folder path containing TXT files: ").strip()

    if not os.path.exists(folder_path):
        print(f"âŒ Error: Folder '{folder_path}' does not exist!")
    elif not os.path.isdir(folder_path):
        print(f"âŒ Error: '{folder_path}' is not a directory!")
    else:
        create_vector_db(folder_path)