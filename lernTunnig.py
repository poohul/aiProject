# hil_data_collector.py
import re
import json
import time
from typing import Dict, Any, List, Union
from datetime import datetime, timezone
from dateutil.relativedelta import relativedelta
from pathlib import Path

# --- RAG ì„¤ì • ìƒìˆ˜ ---
DB_FOLDER = "./chroma_db3"
INITIAL_K = 100  # HIL ëª¨ë“œì—ì„œëŠ” ë„“ì€ ë²”ìœ„ ê²€ìƒ‰ì„ ìœ„í•´ Kë¥¼ í¬ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.
# ----------------------

# --- í•™ìŠµ ë°ì´í„° ì €ì¥ ê²½ë¡œ ---
HIL_DATA_DIR = Path("./hil_training_data")
# ------------------------------

# --- Langchain Imports ---
try:
    from langchain_chroma import Chroma
except Exception:
    from langchain_community.vectorstores import Chroma

try:
    from langchain_huggingface import HuggingFaceEmbeddings
except Exception:
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
    except Exception:
        raise ImportError("HuggingFaceEmbeddings import failed. Check dependencies.")

# --- ì „ì—­ ë³€ìˆ˜ (HIL ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ë¡œì§ ìœ ì§€ë¥¼ ìœ„í•´ ì„ ì–¸)
use_gpu = False


# ---------- ë²¡í„° DB ë¡œë“œ ----------
def load_vector_db(persist_dir: str = DB_FOLDER):
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    return db


# ---------- íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ í•¨ìˆ˜ ----------
def conv_timestamp(timestamp):
    date_str = 'ì•Œ ìˆ˜ ì—†ìŒ'
    if isinstance(timestamp, (int, float)) and timestamp > 0:
        try:
            date_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
        except Exception:
            date_str = 'ë³€í™˜ ì˜¤ë¥˜'
    return date_str


# ---------- í•„í„° ì¶”ì¶œ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©) ----------
# LLM í˜¸ì¶œì´ ì—†ìœ¼ë¯€ë¡œ, LLM/QA ì²´ì¸ ê´€ë ¨ í•¨ìˆ˜ëŠ” ì œê±°í•˜ê³  í•µì‹¬ í•¨ìˆ˜ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
def extract_chroma_filter(query: str) -> tuple[Union[Dict[str, Any], None], Union[str, None]]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ChromaDB ê²€ìƒ‰ì„ ìœ„í•œ í•„í„°ë§ ì¸ìì™€ ì œëª© í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ê¸°ì¡´ extract_chroma_filter í•¨ìˆ˜ ë‚´ìš© ê·¸ëŒ€ë¡œ í¬í•¨)
    """
    now_utc = datetime.now(timezone.utc)
    where_conditions: List[Dict[str, Any]] = []
    search_kwargs: Dict[str, Any] = {}
    title_keyword = None

    # A. ì œëª© í•„í„°ë§ ë¡œì§ (ìƒëµ)
    title_pattern = re.search(r"(ì œëª©|íƒ€ì´í‹€)[^\s]*\s*(?:(?:ì—|ì´)?\s*(?:í¬í•¨ëœ|ìˆëŠ”)?\s*|.*?\s*)\s*([^\s]+)", query)
    if title_pattern:
        keyword = title_pattern.group(2).strip()
        if keyword:
            title_keyword = keyword
            print(f"ğŸ” ì œëª© í‚¤ì›Œë“œ ê°ì§€: '{keyword}' (Python í›„ì²˜ë¦¬ ì˜ˆì •)")

    # B. ë‚ ì§œ í•„í„°ë§ ë¡œì§ (ìƒëµ)
    after_year_pattern = re.search(r"(\d{4})ë…„(?:ë„)?\s*ì´í›„", query)
    if after_year_pattern:
        year = int(after_year_pattern.group(1))
        start_date_utc = datetime(year, 1, 1, 0, 0, 0, tzinfo=timezone.utc)
        start_timestamp = start_date_utc.timestamp()
        where_conditions.append({"date": {"$gte": start_timestamp}})

    within_month_pattern = re.search(r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(?:ì´ë‚´|ë‚´|ê¹Œì§€)", query)
    if within_month_pattern:
        year = int(within_month_pattern.group(1))
        month = int(within_month_pattern.group(2))
        next_month_start = datetime(year, month + 1, 1, 0, 0, 0, tzinfo=timezone.utc) if month < 12 else datetime(
            year + 1, 1, 1, 0, 0, 0, tzinfo=timezone.utc)
        end_timestamp_exclusive = next_month_start.timestamp()
        where_conditions.append({"date": {"$lt": end_timestamp_exclusive}})

    last_months_pattern = re.search(r"ì§€ë‚œ\s*(\d+)\s*ê°œì›”", query)
    if last_months_pattern:
        months = int(last_months_pattern.group(1))
        start_date_limit = now_utc - relativedelta(months=months)
        start_timestamp = start_date_limit.timestamp()
        where_conditions.append({"date": {"$gte": start_timestamp}})

    last_years_pattern = re.search(r"ì§€ë‚œ\s*(\d+)\s*ë…„", query)
    if last_years_pattern:
        years = int(last_years_pattern.group(1))
        start_date_limit = now_utc - relativedelta(years=years)
        start_timestamp = start_date_limit.timestamp()
        where_conditions.append({"date": {"$gte": start_timestamp}})

    # C. ìµœì¢… í•„í„° êµ¬ì¡° ì¡°ë¦½ (ìƒëµ)
    if where_conditions:
        search_kwargs["where"] = where_conditions[0] if len(where_conditions) == 1 else {"$and": where_conditions}

    return (search_kwargs if search_kwargs else None, title_keyword)


# ---------- HIL ë°ì´í„° ìˆ˜ì§‘ ë©”ì¸ ë¡œì§ ----------
def interactive_labeling_mode(db, query: str):
    """
    ì‚¬ìš©ìì—ê²Œ K=100 ë¬¸ì„œë¥¼ ë³´ì—¬ì£¼ê³  ì •ë‹µì„ ì„ íƒí•˜ë„ë¡ ìœ ë„í•˜ë©° ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """

    # 1. ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° í•„í„°ì™€ ì œëª© í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
    metadata_filter, title_keyword = extract_chroma_filter(query)
    current_k = INITIAL_K

    # 2. ë¬¸ì„œ ê²€ìƒ‰ (Initial Retrieval)
    try:
        if metadata_filter and 'where' in metadata_filter:
            where_condition = metadata_filter['where']
            docs = db.similarity_search(query=query, k=current_k, filter=where_condition)
        else:
            docs = db.similarity_search(query=query, k=current_k)

    except Exception as e:
        print(f"âš ï¸ ChromaDB ê²€ìƒ‰ ì˜¤ë¥˜ ë°œìƒ. í•„í„° ì—†ì´ ì¬ì‹œë„: {e}")
        docs = db.similarity_search(query=query, k=current_k)

    # 3. ì œëª© í‚¤ì›Œë“œë¡œ Pythonì—ì„œ í›„ì²˜ë¦¬ í•„í„°ë§ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if title_keyword:
        original_count = len(docs)
        keyword_lower = title_keyword.lower().strip()
        filtered_docs = []
        for d in docs:
            title = d.metadata.get('title', '').lower().strip()
            if keyword_lower in title:
                filtered_docs.append(d)
        docs = filtered_docs
        print(f"ğŸ” ì œëª© '{title_keyword}' í•„í„° ì ìš©: {original_count}ê°œ â†’ {len(docs)}ê°œ ë¬¸ì„œ")

    if not docs:
        print("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì–´ ë¼ë²¨ë§ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\n--- ğŸ§  Active Learning: ì •ë‹µ ë¬¸ì„œ ì„ íƒ ({len(docs)}ê°œ ì¤‘) ---")

    # 4. ì‚¬ìš©ìì—ê²Œ ë¬¸ì„œ ëª©ë¡ ë³´ì—¬ì£¼ê¸°
    for i, d in enumerate(docs, 1):
        title = d.metadata.get("title", "ì œëª© ì—†ìŒ")
        date_str = conv_timestamp(d.metadata.get("date", "ë‚ ì§œ ì—†ìŒ"))
        source = d.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")
        snippet = d.page_content[:100].replace("\n", " ")
        print(f"[{i}] ì œëª©: {title} | ë‚ ì§œ: {date_str} | ì¶œì²˜: {source} | ë‚´ìš©: {snippet}...")

    # 5. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    try:
        selection = input("\nğŸ’¡ ê°€ì¥ ì •í™•í•œ ë‹µë³€ ë¬¸ì„œì˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì·¨ì†Œ: 0): ").strip()
        doc_index = int(selection) - 1

        if doc_index < 0 or doc_index >= len(docs):
            print("ë¼ë²¨ë§ ì·¨ì†Œ ë˜ëŠ” ì˜ëª»ëœ ë²ˆí˜¸ ì…ë ¥.")
            return

        # 6. ê¸ì • ìŒ (Positive) ë° ë¶€ì • ìŒ (Negative) ìë™ êµ¬ì¶•
        positive_doc = docs[doc_index]
        print(f"\nâœ… ì •ë‹µ ë¬¸ì„œ ì„ íƒë¨: {positive_doc.metadata.get('title')}")

        # ë‚˜ë¨¸ì§€ ë¬¸ì„œë¥¼ ë¶€ì • ìŒìœ¼ë¡œ ê°„ì£¼ (Top-20 ì˜¤ë‹µë§Œ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì )
        # 100ê°œ ì¤‘ ì •ë‹µì„ ëº€ ë‚˜ë¨¸ì§€ 19ê°œ ë¬¸ì„œ (Top-20ì—ì„œ)ë¥¼ Hard Negativesë¡œ ì‚¬ìš©
        negative_docs = [d for i, d in enumerate(docs) if i != doc_index and i < 20]

        # 7. íŠ¸ë¦½ë › ë°ì´í„° ì €ì¥
        HIL_DATA_DIR.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # íŒŒì¼ëª…ì— ì§ˆë¬¸ ë‚´ìš©ì„ í¬í•¨ (íŒŒì¼ëª… ì•ˆì „ ì²˜ë¦¬)
        safe_query = re.sub(r'[\\/:*?"<>|]', '', query).strip()[:30]  # íŒŒì¼ëª… ê¸¸ì´ë¥¼ 30ìë¡œ ì œí•œ
        data_file = HIL_DATA_DIR / f"triplet_{safe_query}_{timestamp}.json"

        # ì €ì¥í•  ë°ì´í„° êµ¬ì¡°
        triplet_data = {
            "query": query,
            "positive": {
                "content": positive_doc.page_content,
                "title": positive_doc.metadata.get("title", "N/A"),
                "source": positive_doc.metadata.get("source", "N/A"),
            },
            "negatives": [
                {"content": d.page_content, "title": d.metadata.get("title", "N/A")}
                for d in negative_docs
            ]
        }

        with open(data_file, 'w', encoding='utf-8') as f:
            # ensure_ascii=False: í•œê¸€ ê¹¨ì§ ë°©ì§€
            json.dump(triplet_data, f, ensure_ascii=False, indent=4)

        print(f"ğŸ’¾ í•™ìŠµ ë°ì´í„° íŠ¸ë¦½ë ›ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {data_file}")
        print(f"    (ë¶€ì • ë¬¸ì„œ {len(negative_docs)}ê°œ í¬í•¨)")

    except ValueError:
        print("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ì…ë‹ˆë‹¤. ìˆ«ìë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âš ï¸ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ---------- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (HIL ëª¨ë“œ ì „ìš©) ----------
def main():
    print("ğŸ§  ëŠ¥ë™ í•™ìŠµ(HIL) ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ ì‹œì‘\n")

    db = load_vector_db()

    while True:
        try:
            query = input("ğŸ—¨ï¸ í•™ìŠµ ë°ì´í„°ë¡œ ë§Œë“¤ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (Ctrl+C ë¡œ ì¢…ë£Œ): ").strip()
            if not query:
                continue

            interactive_labeling_mode(db, query)

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


if __name__ == "__main__":
    main()